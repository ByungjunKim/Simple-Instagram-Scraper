{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n",
      "[WDM] - Driver [/home/bjkim/.wdm/drivers/chromedriver/linux64/90.0.4430.24/chromedriver] found in cache\n",
      "\n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n",
      "[WDM] - Driver [/home/bjkim/.wdm/drivers/chromedriver/linux64/90.0.4430.24/chromedriver] found in cache\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# Simple-Instagram-Scraper v1.0.3\n",
    "# Release: 10.11.2020\n",
    "# GitHub: do-me\n",
    "#################################\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd, numpy as np\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random \n",
    "import string\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')\n",
    "option.add_argument('window-size=1920x1080')\n",
    "option.add_argument(\"disable-gpu\")\n",
    "# 혹은 options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# driver = webdriver.Chrome('chromedriver', chrome_options=option)\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options=option)\n",
    "\n",
    "chrome_prefs = {}\n",
    "option.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "chrome_prefs[\"profile.managed_default_content_settings\"] = {\"images\": 2}\n",
    "\n",
    "# Parameters\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# CRUCIAL PARAMS\n",
    "\n",
    "# directory to chromedriver or geckodriver, find chromedriver here: https://chromedriver.chromium.org/\n",
    "browser = webdriver.Chrome(ChromeDriverManager().install(),options=option)\n",
    "# browser = webdriver.Chrome(\"yourpath/chromedriver.exe\", options=option)\n",
    "\n",
    "# user credentials\n",
    "username = \"dkfndjgjdj\"\n",
    "userpassword = \"skkukor!!\"\n",
    "\n",
    "# which page?\n",
    "pagetoscrape = \"https://www.instagram.com/explore/tags/%EB%8C%80%ED%95%99%EC%9B%90%EC%83%9D%EC%9D%BC%EA%B8%B0/\" # either hashtag, location id or user account possible\n",
    "# pagetoscrape = \"https://www.instagram.com/explore/tags/berlin/\"\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# OPTIONAL PARAMS\n",
    "\n",
    "# maximum posts to scrape\n",
    "maxiter = 10000\n",
    "\n",
    "# filename for output csv\n",
    "out_csv = \"-\".join(pagetoscrape.split(\"/\")[-3:])[:-1] + \".csv\" # i.e.'118546-thessaloniki' for locations or tags-berlin for hashtags\n",
    "\n",
    "# quite crucial but subject to trial and error due to unknown Instagram blocking policy: breaks\n",
    "# set a random break duration for every iteration after opening one post and before going to the next one\n",
    "short_pauseduration_min = 2 # seconds \n",
    "short_pauseduration_max = 2.7 # seconds\n",
    "\n",
    "# set a random break duration for longer breaks...\n",
    "long_pauseduration_min = 4.8 # seconds \n",
    "long_pauseduration_max = 10.5 # seconds\n",
    "\n",
    "# ...for the following random iterations (number of iterations = index of scraped posts)\n",
    "pauselist = random.sample(range(10, 10000), 400) # between 10 and 10000 generate list of 500 values # randomList.sort() for sorting\n",
    "pauselist.append([x + int(random.uniform(1,10)) for x in np.arange(20, 10000, 50).tolist()]) # just to make sure: add list with values every 51th to 60th iteration  \n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# ADVANCED PARAMS\n",
    "\n",
    "# uncomment for saving in a folder called \"data\" you created before manually\n",
    "# if os.getcwd().split(\"\\\\\")[-1] != \"data\": # if you changed working dir already for some reason\n",
    "#  os.chdir(r\"data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### some functions #####\n",
    "\n",
    "def delete_cache(first=False):\n",
    "     \n",
    "    browser.execute_script(\"window.open('');\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    browser.switch_to.window(browser.window_handles[-1])\n",
    "    time.sleep(2)\n",
    "    \n",
    "    browser.get('chrome://settings/clearBrowserData')\n",
    "    time.sleep(2)\n",
    "    \n",
    "    actions = ActionChains(browser) \n",
    "    actions.send_keys(Keys.TAB * 2) # send right combination\n",
    "    actions.perform()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    actions = ActionChains(browser) \n",
    "    actions.send_keys(Keys.DOWN * 4) # send right combination\n",
    "    actions.perform()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    actions = ActionChains(browser) \n",
    "    actions.send_keys(Keys.TAB * 2) # send right combination\n",
    "    actions.perform()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if first == True: # only first time, chrome saves input from first time\n",
    "        actions = ActionChains(browser) \n",
    "        actions.send_keys(Keys.SPACE) # send right combination\n",
    "        actions.perform()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    actions = ActionChains(browser) \n",
    "    actions.send_keys(Keys.TAB * 3 + Keys.ENTER) # confirm\n",
    "    actions.perform()\n",
    "    time.sleep(15) # wait some time to finish\n",
    "    \n",
    "    browser.close() # close this tab\n",
    "    browser.switch_to.window(browser.window_handles[0]) # switch back\n",
    "\n",
    "def brsel(obj): # browser select, convenience function\n",
    "    global browser\n",
    "    try:\n",
    "        return browser.find_elements_by_css_selector(obj)\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def long_pauseduration():\n",
    "    return random.uniform(long_pauseduration_min, long_pauseduration_max)\n",
    "\n",
    "def short_pauseduration():\n",
    "    return random.uniform(short_pauseduration_min, short_pauseduration_max)\n",
    "\n",
    "def nextpost():\n",
    "    brsel(\".coreSpriteRightPaginationArrow\")[0].click()\n",
    "\n",
    "def previouspost():\n",
    "    brsel(\".coreSpriteLeftPaginationArrow\")[0].click()\n",
    "\n",
    "def gibberish():\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for _ in range(10))\n",
    "\n",
    "def parsepost(): # parse function when post is displayed in gallery view, could also be executed manually\n",
    "    global browser\n",
    "    \n",
    "    # values always existent\n",
    "    p_time = brsel('time')[0].get_attribute('datetime')\n",
    "    p_user_name = brsel('.ZIAjV')[0].text \n",
    "    p_user_id = brsel('.ZIAjV')[0].get_attribute('href').split(\"/\")[-2] \n",
    "    p_post_id = browser.current_url.split(\"/\")[-2]\n",
    "\n",
    "    # values sometimes existent wrap in try except function\n",
    "    p_loc_name = np.nan\n",
    "    p_loc_slug = np.nan\n",
    "    p_loc_id = np.nan\n",
    "    p_likes = np.nan\n",
    "    p_clicks = np.nan\n",
    "    p_text = np.nan\n",
    "    p_hashtags = np.nan\n",
    "    p_alt = np.nan\n",
    "    \n",
    "    if len(brsel('.O4GlU'))>0: # if location exists\n",
    "        p_loc_name = brsel('.O4GlU')[0].text\n",
    "        p_loc_slug = brsel('.O4GlU')[0].get_attribute('href').split(\"/\")[-2]\n",
    "        p_loc_id = brsel('.O4GlU')[0].get_attribute('href').split(\"/\")[-3]\n",
    "        \n",
    "    # if len(brsel('.Nm9Fw button span'))>0: # likes\n",
    "    #     p_likes = brsel('.Nm9Fw button span')[0].text # likes # can be non\n",
    "    if len(brsel('.Nm9Fw a span'))>0: # likes\n",
    "        p_likes = brsel('.Nm9Fw a span')[0].text # likes # can be non\n",
    "           \n",
    "    if len(brsel('.vcOH2'))>0: # clicks\n",
    "        p_clicks = brsel('.vcOH2')[0].text  \n",
    "        \n",
    "    if len(brsel('ul li'))>0:\n",
    "        p_text = brsel('ul li span')[1].text # full text including hashtags\n",
    "        if len(brsel('ul li')[0].find_elements_by_css_selector('span')) > 0: # only hashtags\n",
    "            if len(brsel('ul li span')[1].find_elements_by_css_selector('a'))>0:\n",
    "                p_hashtags = [x.text for x in brsel('ul li span')[1].find_elements_by_css_selector('a')]  \n",
    "    \n",
    "    if len(brsel('._2dDPU .KL4Bh img'))>0:\n",
    "        try: \n",
    "            p_alt = brsel('._2dDPU .KL4Bh img')[0].get_attribute('alt')\n",
    "        except:\n",
    "            p_alt = np.nan\n",
    "    \n",
    "    return [p_time,p_user_name,p_user_id,p_post_id,p_loc_name,p_loc_slug,p_loc_id,p_likes,p_clicks,p_text,p_hashtags,p_alt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login\n",
    "browser.get('https://www.instagram.com/accounts/login/') # open login page and log in\n",
    "time.sleep(3)\n",
    "# browser.find_elements_by_css_selector('.bIiDR')[0].click() # confirm cookies\n",
    "time.sleep(1)\n",
    "\n",
    "emailInput = browser.find_elements_by_css_selector('form input')[0]\n",
    "passwordInput = browser.find_elements_by_css_selector('form input')[1]\n",
    "\n",
    "emailInput.send_keys(username)\n",
    "passwordInput.send_keys(userpassword)\n",
    "passwordInput.send_keys(Keys.ENTER)\n",
    "\n",
    "time.sleep(3)\n",
    "browser.find_elements_by_css_selector('button')[1].click() # dont save user credentials (not possible)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize page\n",
    "browser.get(pagetoscrape)\n",
    "time.sleep(2)\n",
    "brsel('._9AhH0')[9].click()  # get first post ignoring top posts\n",
    "\n",
    "# until here one could go manually correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [] # emtpy list \n",
    "# execute this cell only once in the beginning \n",
    "# if for some reason the loop in the next cell stops, you can simple reexecute the next cell\n",
    "# this list then contains the previously scraped information and adds new posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "Ended at iteration:62\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i < maxiter:\n",
    "    print(\"Iteration: \" + str(i), end=\"\\r\")\n",
    "    \n",
    "    # if not loading go back and forth\n",
    "    # if fails again, set np.nan\n",
    "    try: \n",
    "        currpost = parsepost()\n",
    "    except IndexError:\n",
    "        previouspost()\n",
    "        time.sleep(3)\n",
    "        nextpost()\n",
    "        time.sleep(3)\n",
    "        try: \n",
    "            currpost = parsepost()\n",
    "        except IndexError:\n",
    "            p_post_id = browser.current_url.split(\"/\")[-2]\n",
    "            posts.append([np.nan,np.nan,np.nan,p_post_id,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "            if i in pauselist:\n",
    "                time.sleep(long_pauseduration())\n",
    "\n",
    "            if len(brsel(\".coreSpriteRightPaginationArrow\"))==0: # break if come to the end or blocked entirely\n",
    "                print(\"Ended at iteration:\" +str(i))\n",
    "                break\n",
    "\n",
    "            nextpost()\n",
    "            time.sleep(short_pauseduration())\n",
    "            i += 1\n",
    "            continue\n",
    "    \n",
    "    posts.append(currpost)\n",
    "    \n",
    "    if i in pauselist:\n",
    "        time.sleep(long_pauseduration())\n",
    "    \n",
    "    if len(brsel(\".coreSpriteRightPaginationArrow\"))==0: # break if come to the end or blocked entirely\n",
    "        print(\"Ended at iteration:\" +str(i))\n",
    "        break\n",
    "        \n",
    "    if i % 11 == 0: # every nth iteration\n",
    "        try:\n",
    "            brsel('form')[0].click()\n",
    "            try:\n",
    "                brsel('form textarea')[0].send_keys(gibberish())\n",
    "            except IndexError:\n",
    "                time.sleep(0.3)          \n",
    "        except:\n",
    "            time.sleep(0.2)\n",
    "        time.sleep(1.2)\n",
    "    \n",
    "    if i == 100: # only first time cache settings\n",
    "        delete_cache(first=True)\n",
    "        nextpost()\n",
    "        time.sleep(short_pauseduration())\n",
    "        i += 1\n",
    "        continue\n",
    "        \n",
    "    if i % 100 == 0: # every nth iteration\n",
    "        delete_cache()\n",
    "         \n",
    "    nextpost()\n",
    "    time.sleep(short_pauseduration())\n",
    "    # print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(posts)\n",
    "df.columns = [\"time\",\"user_name\",\"user_id\",\"post_id\",\"loc_name\",\"loc_slug\",\"loc_id\",\"likes\",\"clicks\",\"text\",\"hashtags\",\"alt\"]\n",
    "\n",
    "# check if file already exists\n",
    "if os.path.isfile(out_csv): # if exists append new posts to old file \n",
    "    tf = pd.read_csv(out_csv)\n",
    "    tf = tf.append(df)\n",
    "    tf.to_csv(out_csv,index=False)\n",
    "\n",
    "# else create new file\n",
    "else: \n",
    "    df.to_csv(out_csv,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                        time       user_name         user_id      post_id  \\\n",
       "0   2021-04-01T15:01:24.000Z       mminyy.20       mminyy.20  CNIKvqsHRi9   \n",
       "1   2021-03-27T16:05:40.000Z    hattie_sweet    hattie_sweet  CM7aIALMjWS   \n",
       "2   2020-11-27T13:48:12.000Z      yoon_liana      yoon_liana  CIGLAGgHnH2   \n",
       "3   2020-11-27T12:37:04.000Z       archae_07       archae_07  CIGC3LDjJlJ   \n",
       "4   2020-11-22T02:11:32.000Z  study_platypus  study_platypus  CH4DH1EDxOK   \n",
       "..                       ...             ...             ...          ...   \n",
       "57  2018-06-14T02:57:43.000Z         minjusm         minjusm  Bj_ThAbnfii   \n",
       "58  2017-08-12T16:13:09.000Z      nitrodiary      nitrodiary  BXszTFUhSOZ   \n",
       "59  2017-08-11T15:15:02.000Z      nitrodiary      nitrodiary  BXqH2iOBwfA   \n",
       "60  2017-08-11T15:13:03.000Z      nitrodiary      nitrodiary  BXqHoEIhoQ1   \n",
       "61  2017-08-01T19:57:11.000Z      nitrodiary      nitrodiary  BXQ4MaaBXqV   \n",
       "\n",
       "                          loc_name                   loc_slug  \\\n",
       "0                     아웃백스테이크부천역사점            431066837237911   \n",
       "1                              NaN                        NaN   \n",
       "2   신촌 Sinchon, Seoul, South Korea  sinchon-seoul-south-korea   \n",
       "3                        한국전통문화대학교                  287114523   \n",
       "4                              NaN                        NaN   \n",
       "..                             ...                        ...   \n",
       "57                             NaN                        NaN   \n",
       "58                             NaN                        NaN   \n",
       "59                             NaN                        NaN   \n",
       "60                             NaN                        NaN   \n",
       "61                             NaN                        NaN   \n",
       "\n",
       "             loc_id likes    clicks  \\\n",
       "0         locations    15       NaN   \n",
       "1               NaN   NaN       NaN   \n",
       "2   486650968033082    58       NaN   \n",
       "3         locations    19       NaN   \n",
       "4               NaN   NaN  53 views   \n",
       "..              ...   ...       ...   \n",
       "57              NaN    66       NaN   \n",
       "58              NaN     3       NaN   \n",
       "59              NaN     4       NaN   \n",
       "60              NaN     7       NaN   \n",
       "61              NaN     3       NaN   \n",
       "\n",
       "                                                 text  \\\n",
       "0   📌\\n연구발표 디데이가 다가오면서 피가 마르는 기분이었다 설문은 진즉 돌렸지만 SP...   \n",
       "1   #커피머신이있어도 #카페간드아 #아메하루최소두잔 #혈중카페인농도유지 #대학원생일기 ...   \n",
       "2   얼른 거리두기완화되서 분위기좋은 카페에서 그림 그리고 싶다🙀\\n.\\n#coronao...   \n",
       "3                        홈카페말고 랩카페☕\\n#대학원생일기 #연구실스타그램   \n",
       "4   22 November 2020\\n😐 닥치고 할 뿐 타닥타탁\\n.\\n.\\n.\\n#ph...   \n",
       "..                                                ...   \n",
       "57  .\\n#광고론 #발표\\n발표 끝난 지 하루 만에 또 발표,\\n주변 상황을 보니 뭐라...   \n",
       "58    #workingonweekends #nitrodiary #phdlife #대학원생일기   \n",
       "59                     캬 #nitrodiary #phdlife #대학원생일기   \n",
       "60  sunset on the Libe Slope #throwback #cornell #...   \n",
       "61  #nitrodiary #manuscript #submitted #phdlife #대...   \n",
       "\n",
       "                                             hashtags  \\\n",
       "0   [#논쓰남, #논쓰남통계과외, #논쓰남통계과외후기, #SPSS통계과외, #통계과외,...   \n",
       "1                                                 NaN   \n",
       "2                       [#coronaout, #daily, #대학원생일기]   \n",
       "3                                 [#대학원생일기, #연구실스타그램]   \n",
       "4   [#phdlife, #phdjourney, #대학원생의삶, #박사과정, #대학원생,...   \n",
       "..                                                ...   \n",
       "57                                                NaN   \n",
       "58  [#workingonweekends, #nitrodiary, #phdlife, #대...   \n",
       "59                   [#nitrodiary, #phdlife, #대학원생일기]   \n",
       "60  [#throwback, #cornell, #memories, #nitrodiary,...   \n",
       "61  [#nitrodiary, #manuscript, #submitted, #phdlif...   \n",
       "\n",
       "                                                  alt  \n",
       "0   Photo by 민영(minyoung) in 아웃백스테이크부천역사점 with...  \n",
       "1   Photo by Hattie_sweets on March 27, 2021. May ...  \n",
       "2   Photo by 박지윤 in 신촌 Sinchon, Seoul, South Korea...  \n",
       "3   Photo by Ким ён гиль in 한국전통문화대학교. May be an i...  \n",
       "4                                                 NaN  \n",
       "..                                                ...  \n",
       "57  Photo by 아나운서 김민주 on June 13, 2018. ...  \n",
       "58                Photo by NitroD on August 12, 2017.  \n",
       "59                Photo by NitroD on August 11, 2017.  \n",
       "60                Photo by NitroD on August 11, 2017.  \n",
       "61                Photo by NitroD on August 01, 2017.  \n",
       "\n",
       "[62 rows x 12 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>user_name</th>\n      <th>user_id</th>\n      <th>post_id</th>\n      <th>loc_name</th>\n      <th>loc_slug</th>\n      <th>loc_id</th>\n      <th>likes</th>\n      <th>clicks</th>\n      <th>text</th>\n      <th>hashtags</th>\n      <th>alt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-04-01T15:01:24.000Z</td>\n      <td>mminyy.20</td>\n      <td>mminyy.20</td>\n      <td>CNIKvqsHRi9</td>\n      <td>아웃백스테이크부천역사점</td>\n      <td>431066837237911</td>\n      <td>locations</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>📌\\n연구발표 디데이가 다가오면서 피가 마르는 기분이었다 설문은 진즉 돌렸지만 SP...</td>\n      <td>[#논쓰남, #논쓰남통계과외, #논쓰남통계과외후기, #SPSS통계과외, #통계과외,...</td>\n      <td>Photo by 민영(minyoung) in 아웃백스테이크부천역사점 with...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-03-27T16:05:40.000Z</td>\n      <td>hattie_sweet</td>\n      <td>hattie_sweet</td>\n      <td>CM7aIALMjWS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#커피머신이있어도 #카페간드아 #아메하루최소두잔 #혈중카페인농도유지 #대학원생일기 ...</td>\n      <td>NaN</td>\n      <td>Photo by Hattie_sweets on March 27, 2021. May ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-11-27T13:48:12.000Z</td>\n      <td>yoon_liana</td>\n      <td>yoon_liana</td>\n      <td>CIGLAGgHnH2</td>\n      <td>신촌 Sinchon, Seoul, South Korea</td>\n      <td>sinchon-seoul-south-korea</td>\n      <td>486650968033082</td>\n      <td>58</td>\n      <td>NaN</td>\n      <td>얼른 거리두기완화되서 분위기좋은 카페에서 그림 그리고 싶다🙀\\n.\\n#coronao...</td>\n      <td>[#coronaout, #daily, #대학원생일기]</td>\n      <td>Photo by 박지윤 in 신촌 Sinchon, Seoul, South Korea...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-11-27T12:37:04.000Z</td>\n      <td>archae_07</td>\n      <td>archae_07</td>\n      <td>CIGC3LDjJlJ</td>\n      <td>한국전통문화대학교</td>\n      <td>287114523</td>\n      <td>locations</td>\n      <td>19</td>\n      <td>NaN</td>\n      <td>홈카페말고 랩카페☕\\n#대학원생일기 #연구실스타그램</td>\n      <td>[#대학원생일기, #연구실스타그램]</td>\n      <td>Photo by Ким ён гиль in 한국전통문화대학교. May be an i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-11-22T02:11:32.000Z</td>\n      <td>study_platypus</td>\n      <td>study_platypus</td>\n      <td>CH4DH1EDxOK</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>53 views</td>\n      <td>22 November 2020\\n😐 닥치고 할 뿐 타닥타탁\\n.\\n.\\n.\\n#ph...</td>\n      <td>[#phdlife, #phdjourney, #대학원생의삶, #박사과정, #대학원생,...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>2018-06-14T02:57:43.000Z</td>\n      <td>minjusm</td>\n      <td>minjusm</td>\n      <td>Bj_ThAbnfii</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>66</td>\n      <td>NaN</td>\n      <td>.\\n#광고론 #발표\\n발표 끝난 지 하루 만에 또 발표,\\n주변 상황을 보니 뭐라...</td>\n      <td>NaN</td>\n      <td>Photo by 아나운서 김민주 on June 13, 2018. ...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>2017-08-12T16:13:09.000Z</td>\n      <td>nitrodiary</td>\n      <td>nitrodiary</td>\n      <td>BXszTFUhSOZ</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>#workingonweekends #nitrodiary #phdlife #대학원생일기</td>\n      <td>[#workingonweekends, #nitrodiary, #phdlife, #대...</td>\n      <td>Photo by NitroD on August 12, 2017.</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>2017-08-11T15:15:02.000Z</td>\n      <td>nitrodiary</td>\n      <td>nitrodiary</td>\n      <td>BXqH2iOBwfA</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>캬 #nitrodiary #phdlife #대학원생일기</td>\n      <td>[#nitrodiary, #phdlife, #대학원생일기]</td>\n      <td>Photo by NitroD on August 11, 2017.</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>2017-08-11T15:13:03.000Z</td>\n      <td>nitrodiary</td>\n      <td>nitrodiary</td>\n      <td>BXqHoEIhoQ1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>sunset on the Libe Slope #throwback #cornell #...</td>\n      <td>[#throwback, #cornell, #memories, #nitrodiary,...</td>\n      <td>Photo by NitroD on August 11, 2017.</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>2017-08-01T19:57:11.000Z</td>\n      <td>nitrodiary</td>\n      <td>nitrodiary</td>\n      <td>BXQ4MaaBXqV</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>#nitrodiary #manuscript #submitted #phdlife #대...</td>\n      <td>[#nitrodiary, #manuscript, #submitted, #phdlif...</td>\n      <td>Photo by NitroD on August 01, 2017.</td>\n    </tr>\n  </tbody>\n</table>\n<p>62 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "# display just a couple of rows\n",
    "df\n",
    "\n",
    "# display all rows\n",
    "# pd.set_option('display.max_rows', df.shape[0]+1) # display all rows\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}