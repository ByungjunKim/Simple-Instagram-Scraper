{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n",
      "[WDM] - Driver [/home/bjkim/.wdm/drivers/chromedriver/linux64/90.0.4430.24/chromedriver] found in cache\n",
      "\n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n",
      "[WDM] - Driver [/home/bjkim/.wdm/drivers/chromedriver/linux64/90.0.4430.24/chromedriver] found in cache\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# Simple-Instagram-Scraper v1.0.3\n",
    "# Release: 10.11.2020\n",
    "# GitHub: do-me\n",
    "#################################\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd, numpy as np\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random \n",
    "import string\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')\n",
    "option.add_argument('window-size=1920x1080')\n",
    "option.add_argument(\"disable-gpu\")\n",
    "# í˜¹ì€ options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# driver = webdriver.Chrome('chromedriver', chrome_options=option)\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options=option)\n",
    "\n",
    "chrome_prefs = {}\n",
    "option.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "chrome_prefs[\"profile.managed_default_content_settings\"] = {\"images\": 2}\n",
    "\n",
    "# Parameters\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# CRUCIAL PARAMS\n",
    "\n",
    "# directory to chromedriver or geckodriver, find chromedriver here: https://chromedriver.chromium.org/\n",
    "browser = webdriver.Chrome(ChromeDriverManager().install(),options=option)\n",
    "# browser = webdriver.Chrome(\"yourpath/chromedriver.exe\", options=option)\n",
    "\n",
    "# user credentials\n",
    "username = \"dkfndjgjdj\"\n",
    "userpassword = \"skkukor!!\"\n",
    "\n",
    "# which page?\n",
    "pagetoscrape = \"https://www.instagram.com/explore/tags/%EB%8C%80%ED%95%99%EC%9B%90%EC%83%9D%EC%9D%BC%EA%B8%B0/\" # either hashtag, location id or user account possible\n",
    "# pagetoscrape = \"https://www.instagram.com/explore/tags/berlin/\"\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# OPTIONAL PARAMS\n",
    "\n",
    "# maximum posts to scrape\n",
    "maxiter = 10000\n",
    "\n",
    "# filename for output csv\n",
    "out_csv = \"-\".join(pagetoscrape.split(\"/\")[-3:])[:-1] + \".csv\" # i.e.'118546-thessaloniki' for locations or tags-berlin for hashtags\n",
    "\n",
    "# quite crucial but subject to trial and error due to unknown Instagram blocking policy: breaks\n",
    "# set a random break duration for every iteration after opening one post and before going to the next one\n",
    "short_pauseduration_min = 2 # seconds \n",
    "short_pauseduration_max = 2.7 # seconds\n",
    "\n",
    "# set a random break duration for longer breaks...\n",
    "long_pauseduration_min = 4.8 # seconds \n",
    "long_pauseduration_max = 10.5 # seconds\n",
    "\n",
    "# ...for the following random iterations (number of iterations = index of scraped posts)\n",
    "pauselist = random.sample(range(10, 10000), 400) # between 10 and 10000 generate list of 500 values # randomList.sort() for sorting\n",
    "pauselist.append([x + int(random.uniform(1,10)) for x in np.arange(20, 10000, 50).tolist()]) # just to make sure: add list with values every 51th to 60th iteration  \n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# ADVANCED PARAMS\n",
    "\n",
    "# uncomment for saving in a folder called \"data\" you created before manually\n",
    "# if os.getcwd().split(\"\\\\\")[-1] != \"data\": # if you changed working dir already for some reason\n",
    "#  os.chdir(r\"data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### some functions #####\n",
    "\n",
    "def delete_cache(first=False):\n",
    "     \n",
    "    browser.execute_script(\"window.open('');\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    browser.switch_to.window(browser.window_handles[-1])\n",
    "    time.sleep(2)\n",
    "    \n",
    "    browser.get('chrome://settings/clearBrowserData')\n",
    "    time.sleep(2)\n",
    "    \n",
    "    actions = ActionChains(browser) \n",
    "    actions.send_keys(Keys.TAB * 2) # send right combination\n",
    "    actions.perform()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    actions = ActionChains(browser) \n",
    "    actions.send_keys(Keys.DOWN * 4) # send right combination\n",
    "    actions.perform()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    actions = ActionChains(browser) \n",
    "    actions.send_keys(Keys.TAB * 2) # send right combination\n",
    "    actions.perform()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if first == True: # only first time, chrome saves input from first time\n",
    "        actions = ActionChains(browser) \n",
    "        actions.send_keys(Keys.SPACE) # send right combination\n",
    "        actions.perform()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    actions = ActionChains(browser) \n",
    "    actions.send_keys(Keys.TAB * 3 + Keys.ENTER) # confirm\n",
    "    actions.perform()\n",
    "    time.sleep(15) # wait some time to finish\n",
    "    \n",
    "    browser.close() # close this tab\n",
    "    browser.switch_to.window(browser.window_handles[0]) # switch back\n",
    "\n",
    "def brsel(obj): # browser select, convenience function\n",
    "    global browser\n",
    "    try:\n",
    "        return browser.find_elements_by_css_selector(obj)\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def long_pauseduration():\n",
    "    return random.uniform(long_pauseduration_min, long_pauseduration_max)\n",
    "\n",
    "def short_pauseduration():\n",
    "    return random.uniform(short_pauseduration_min, short_pauseduration_max)\n",
    "\n",
    "def nextpost():\n",
    "    brsel(\".coreSpriteRightPaginationArrow\")[0].click()\n",
    "\n",
    "def previouspost():\n",
    "    brsel(\".coreSpriteLeftPaginationArrow\")[0].click()\n",
    "\n",
    "def gibberish():\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for _ in range(10))\n",
    "\n",
    "def parsepost(): # parse function when post is displayed in gallery view, could also be executed manually\n",
    "    global browser\n",
    "    \n",
    "    # values always existent\n",
    "    p_time = brsel('time')[0].get_attribute('datetime')\n",
    "    p_user_name = brsel('.ZIAjV')[0].text \n",
    "    p_user_id = brsel('.ZIAjV')[0].get_attribute('href').split(\"/\")[-2] \n",
    "    p_post_id = browser.current_url.split(\"/\")[-2]\n",
    "\n",
    "    # values sometimes existent wrap in try except function\n",
    "    p_loc_name = np.nan\n",
    "    p_loc_slug = np.nan\n",
    "    p_loc_id = np.nan\n",
    "    p_likes = np.nan\n",
    "    p_clicks = np.nan\n",
    "    p_text = np.nan\n",
    "    p_hashtags = np.nan\n",
    "    p_alt = np.nan\n",
    "    \n",
    "    if len(brsel('.O4GlU'))>0: # if location exists\n",
    "        p_loc_name = brsel('.O4GlU')[0].text\n",
    "        p_loc_slug = brsel('.O4GlU')[0].get_attribute('href').split(\"/\")[-2]\n",
    "        p_loc_id = brsel('.O4GlU')[0].get_attribute('href').split(\"/\")[-3]\n",
    "        \n",
    "    # if len(brsel('.Nm9Fw button span'))>0: # likes\n",
    "    #     p_likes = brsel('.Nm9Fw button span')[0].text # likes # can be non\n",
    "    if len(brsel('.Nm9Fw a span'))>0: # likes\n",
    "        p_likes = brsel('.Nm9Fw a span')[0].text # likes # can be non\n",
    "           \n",
    "    if len(brsel('.vcOH2'))>0: # clicks\n",
    "        p_clicks = brsel('.vcOH2')[0].text  \n",
    "        \n",
    "    if len(brsel('ul li'))>0:\n",
    "        p_text = brsel('ul li span')[1].text # full text including hashtags\n",
    "        if len(brsel('ul li')[0].find_elements_by_css_selector('span')) > 0: # only hashtags\n",
    "            if len(brsel('ul li span')[1].find_elements_by_css_selector('a'))>0:\n",
    "                p_hashtags = [x.text for x in brsel('ul li span')[1].find_elements_by_css_selector('a')]  \n",
    "    \n",
    "    if len(brsel('._2dDPU .KL4Bh img'))>0:\n",
    "        try: \n",
    "            p_alt = brsel('._2dDPU .KL4Bh img')[0].get_attribute('alt')\n",
    "        except:\n",
    "            p_alt = np.nan\n",
    "    \n",
    "    return [p_time,p_user_name,p_user_id,p_post_id,p_loc_name,p_loc_slug,p_loc_id,p_likes,p_clicks,p_text,p_hashtags,p_alt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login\n",
    "browser.get('https://www.instagram.com/accounts/login/') # open login page and log in\n",
    "time.sleep(3)\n",
    "# browser.find_elements_by_css_selector('.bIiDR')[0].click() # confirm cookies\n",
    "time.sleep(1)\n",
    "\n",
    "emailInput = browser.find_elements_by_css_selector('form input')[0]\n",
    "passwordInput = browser.find_elements_by_css_selector('form input')[1]\n",
    "\n",
    "emailInput.send_keys(username)\n",
    "passwordInput.send_keys(userpassword)\n",
    "passwordInput.send_keys(Keys.ENTER)\n",
    "\n",
    "time.sleep(3)\n",
    "browser.find_elements_by_css_selector('button')[1].click() # dont save user credentials (not possible)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize page\n",
    "browser.get(pagetoscrape)\n",
    "time.sleep(2)\n",
    "brsel('._9AhH0')[9].click()  # get first post ignoring top posts\n",
    "\n",
    "# until here one could go manually correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [] # emtpy list \n",
    "# execute this cell only once in the beginning \n",
    "# if for some reason the loop in the next cell stops, you can simple reexecute the next cell\n",
    "# this list then contains the previously scraped information and adds new posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "Ended at iteration:62\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i < maxiter:\n",
    "    print(\"Iteration: \" + str(i), end=\"\\r\")\n",
    "    \n",
    "    # if not loading go back and forth\n",
    "    # if fails again, set np.nan\n",
    "    try: \n",
    "        currpost = parsepost()\n",
    "    except IndexError:\n",
    "        previouspost()\n",
    "        time.sleep(3)\n",
    "        nextpost()\n",
    "        time.sleep(3)\n",
    "        try: \n",
    "            currpost = parsepost()\n",
    "        except IndexError:\n",
    "            p_post_id = browser.current_url.split(\"/\")[-2]\n",
    "            posts.append([np.nan,np.nan,np.nan,p_post_id,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "            if i in pauselist:\n",
    "                time.sleep(long_pauseduration())\n",
    "\n",
    "            if len(brsel(\".coreSpriteRightPaginationArrow\"))==0: # break if come to the end or blocked entirely\n",
    "                print(\"Ended at iteration:\" +str(i))\n",
    "                break\n",
    "\n",
    "            nextpost()\n",
    "            time.sleep(short_pauseduration())\n",
    "            i += 1\n",
    "            continue\n",
    "    \n",
    "    posts.append(currpost)\n",
    "    \n",
    "    if i in pauselist:\n",
    "        time.sleep(long_pauseduration())\n",
    "    \n",
    "    if len(brsel(\".coreSpriteRightPaginationArrow\"))==0: # break if come to the end or blocked entirely\n",
    "        print(\"Ended at iteration:\" +str(i))\n",
    "        break\n",
    "        \n",
    "    if i % 11 == 0: # every nth iteration\n",
    "        try:\n",
    "            brsel('form')[0].click()\n",
    "            try:\n",
    "                brsel('form textarea')[0].send_keys(gibberish())\n",
    "            except IndexError:\n",
    "                time.sleep(0.3)          \n",
    "        except:\n",
    "            time.sleep(0.2)\n",
    "        time.sleep(1.2)\n",
    "    \n",
    "    if i == 100: # only first time cache settings\n",
    "        delete_cache(first=True)\n",
    "        nextpost()\n",
    "        time.sleep(short_pauseduration())\n",
    "        i += 1\n",
    "        continue\n",
    "        \n",
    "    if i % 100 == 0: # every nth iteration\n",
    "        delete_cache()\n",
    "         \n",
    "    nextpost()\n",
    "    time.sleep(short_pauseduration())\n",
    "    # print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(posts)\n",
    "df.columns = [\"time\",\"user_name\",\"user_id\",\"post_id\",\"loc_name\",\"loc_slug\",\"loc_id\",\"likes\",\"clicks\",\"text\",\"hashtags\",\"alt\"]\n",
    "\n",
    "# check if file already exists\n",
    "if os.path.isfile(out_csv): # if exists append new posts to old file \n",
    "    tf = pd.read_csv(out_csv)\n",
    "    tf = tf.append(df)\n",
    "    tf.to_csv(out_csv,index=False)\n",
    "\n",
    "# else create new file\n",
    "else: \n",
    "    df.to_csv(out_csv,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                        time       user_name         user_id      post_id  \\\n",
       "0   2021-04-01T15:01:24.000Z       mminyy.20       mminyy.20  CNIKvqsHRi9   \n",
       "1   2021-03-27T16:05:40.000Z    hattie_sweet    hattie_sweet  CM7aIALMjWS   \n",
       "2   2020-11-27T13:48:12.000Z      yoon_liana      yoon_liana  CIGLAGgHnH2   \n",
       "3   2020-11-27T12:37:04.000Z       archae_07       archae_07  CIGC3LDjJlJ   \n",
       "4   2020-11-22T02:11:32.000Z  study_platypus  study_platypus  CH4DH1EDxOK   \n",
       "..                       ...             ...             ...          ...   \n",
       "57  2018-06-14T02:57:43.000Z         minjusm         minjusm  Bj_ThAbnfii   \n",
       "58  2017-08-12T16:13:09.000Z      nitrodiary      nitrodiary  BXszTFUhSOZ   \n",
       "59  2017-08-11T15:15:02.000Z      nitrodiary      nitrodiary  BXqH2iOBwfA   \n",
       "60  2017-08-11T15:13:03.000Z      nitrodiary      nitrodiary  BXqHoEIhoQ1   \n",
       "61  2017-08-01T19:57:11.000Z      nitrodiary      nitrodiary  BXQ4MaaBXqV   \n",
       "\n",
       "                          loc_name                   loc_slug  \\\n",
       "0                     ì•„ì›ƒë°±ìŠ¤í…Œì´í¬ë¶€ì²œì—­ì‚¬ì             431066837237911   \n",
       "1                              NaN                        NaN   \n",
       "2   ì‹ ì´Œ Sinchon, Seoul, South Korea  sinchon-seoul-south-korea   \n",
       "3                        í•œêµ­ì „í†µë¬¸í™”ëŒ€í•™êµ                  287114523   \n",
       "4                              NaN                        NaN   \n",
       "..                             ...                        ...   \n",
       "57                             NaN                        NaN   \n",
       "58                             NaN                        NaN   \n",
       "59                             NaN                        NaN   \n",
       "60                             NaN                        NaN   \n",
       "61                             NaN                        NaN   \n",
       "\n",
       "             loc_id likes    clicks  \\\n",
       "0         locations    15       NaN   \n",
       "1               NaN   NaN       NaN   \n",
       "2   486650968033082    58       NaN   \n",
       "3         locations    19       NaN   \n",
       "4               NaN   NaN  53 views   \n",
       "..              ...   ...       ...   \n",
       "57              NaN    66       NaN   \n",
       "58              NaN     3       NaN   \n",
       "59              NaN     4       NaN   \n",
       "60              NaN     7       NaN   \n",
       "61              NaN     3       NaN   \n",
       "\n",
       "                                                 text  \\\n",
       "0   ğŸ“Œ\\nì—°êµ¬ë°œí‘œ ë””ë°ì´ê°€ ë‹¤ê°€ì˜¤ë©´ì„œ í”¼ê°€ ë§ˆë¥´ëŠ” ê¸°ë¶„ì´ì—ˆë‹¤ ì„¤ë¬¸ì€ ì§„ì¦‰ ëŒë ¸ì§€ë§Œ SP...   \n",
       "1   #ì»¤í”¼ë¨¸ì‹ ì´ìˆì–´ë„ #ì¹´í˜ê°„ë“œì•„ #ì•„ë©”í•˜ë£¨ìµœì†Œë‘ì” #í˜ˆì¤‘ì¹´í˜ì¸ë†ë„ìœ ì§€ #ëŒ€í•™ì›ìƒì¼ê¸° ...   \n",
       "2   ì–¼ë¥¸ ê±°ë¦¬ë‘ê¸°ì™„í™”ë˜ì„œ ë¶„ìœ„ê¸°ì¢‹ì€ ì¹´í˜ì—ì„œ ê·¸ë¦¼ ê·¸ë¦¬ê³  ì‹¶ë‹¤ğŸ™€\\n.\\n#coronao...   \n",
       "3                        í™ˆì¹´í˜ë§ê³  ë©ì¹´í˜â˜•\\n#ëŒ€í•™ì›ìƒì¼ê¸° #ì—°êµ¬ì‹¤ìŠ¤íƒ€ê·¸ë¨   \n",
       "4   22 November 2020\\nğŸ˜ ë‹¥ì¹˜ê³  í•  ë¿ íƒ€ë‹¥íƒ€íƒ\\n.\\n.\\n.\\n#ph...   \n",
       "..                                                ...   \n",
       "57  .\\n#ê´‘ê³ ë¡  #ë°œí‘œ\\në°œí‘œ ëë‚œ ì§€ í•˜ë£¨ ë§Œì— ë˜ ë°œí‘œ,\\nì£¼ë³€ ìƒí™©ì„ ë³´ë‹ˆ ë­ë¼...   \n",
       "58    #workingonweekends #nitrodiary #phdlife #ëŒ€í•™ì›ìƒì¼ê¸°   \n",
       "59                     ìº¬ #nitrodiary #phdlife #ëŒ€í•™ì›ìƒì¼ê¸°   \n",
       "60  sunset on the Libe Slope #throwback #cornell #...   \n",
       "61  #nitrodiary #manuscript #submitted #phdlife #ëŒ€...   \n",
       "\n",
       "                                             hashtags  \\\n",
       "0   [#ë…¼ì“°ë‚¨, #ë…¼ì“°ë‚¨í†µê³„ê³¼ì™¸, #ë…¼ì“°ë‚¨í†µê³„ê³¼ì™¸í›„ê¸°, #SPSSí†µê³„ê³¼ì™¸, #í†µê³„ê³¼ì™¸,...   \n",
       "1                                                 NaN   \n",
       "2                       [#coronaout, #daily, #ëŒ€í•™ì›ìƒì¼ê¸°]   \n",
       "3                                 [#ëŒ€í•™ì›ìƒì¼ê¸°, #ì—°êµ¬ì‹¤ìŠ¤íƒ€ê·¸ë¨]   \n",
       "4   [#phdlife, #phdjourney, #ëŒ€í•™ì›ìƒì˜ì‚¶, #ë°•ì‚¬ê³¼ì •, #ëŒ€í•™ì›ìƒ,...   \n",
       "..                                                ...   \n",
       "57                                                NaN   \n",
       "58  [#workingonweekends, #nitrodiary, #phdlife, #ëŒ€...   \n",
       "59                   [#nitrodiary, #phdlife, #ëŒ€í•™ì›ìƒì¼ê¸°]   \n",
       "60  [#throwback, #cornell, #memories, #nitrodiary,...   \n",
       "61  [#nitrodiary, #manuscript, #submitted, #phdlif...   \n",
       "\n",
       "                                                  alt  \n",
       "0   Photo by á„†á…µá†«á„‹á…§á†¼(minyoung) in ì•„ì›ƒë°±ìŠ¤í…Œì´í¬ë¶€ì²œì—­ì‚¬ì  with...  \n",
       "1   Photo by Hattie_sweets on March 27, 2021. May ...  \n",
       "2   Photo by ë°•ì§€ìœ¤ in ì‹ ì´Œ Sinchon, Seoul, South Korea...  \n",
       "3   Photo by ĞšĞ¸Ğ¼ Ñ‘Ğ½ Ğ³Ğ¸Ğ»ÑŒ in í•œêµ­ì „í†µë¬¸í™”ëŒ€í•™êµ. May be an i...  \n",
       "4                                                 NaN  \n",
       "..                                                ...  \n",
       "57  Photo by á„‹á…¡á„‚á…¡á„‹á…®á†«á„‰á…¥ á„€á…µá†·á„†á…µá†«á„Œá…® on June 13, 2018. ...  \n",
       "58                Photo by NitroD on August 12, 2017.  \n",
       "59                Photo by NitroD on August 11, 2017.  \n",
       "60                Photo by NitroD on August 11, 2017.  \n",
       "61                Photo by NitroD on August 01, 2017.  \n",
       "\n",
       "[62 rows x 12 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>user_name</th>\n      <th>user_id</th>\n      <th>post_id</th>\n      <th>loc_name</th>\n      <th>loc_slug</th>\n      <th>loc_id</th>\n      <th>likes</th>\n      <th>clicks</th>\n      <th>text</th>\n      <th>hashtags</th>\n      <th>alt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-04-01T15:01:24.000Z</td>\n      <td>mminyy.20</td>\n      <td>mminyy.20</td>\n      <td>CNIKvqsHRi9</td>\n      <td>ì•„ì›ƒë°±ìŠ¤í…Œì´í¬ë¶€ì²œì—­ì‚¬ì </td>\n      <td>431066837237911</td>\n      <td>locations</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>ğŸ“Œ\\nì—°êµ¬ë°œí‘œ ë””ë°ì´ê°€ ë‹¤ê°€ì˜¤ë©´ì„œ í”¼ê°€ ë§ˆë¥´ëŠ” ê¸°ë¶„ì´ì—ˆë‹¤ ì„¤ë¬¸ì€ ì§„ì¦‰ ëŒë ¸ì§€ë§Œ SP...</td>\n      <td>[#ë…¼ì“°ë‚¨, #ë…¼ì“°ë‚¨í†µê³„ê³¼ì™¸, #ë…¼ì“°ë‚¨í†µê³„ê³¼ì™¸í›„ê¸°, #SPSSí†µê³„ê³¼ì™¸, #í†µê³„ê³¼ì™¸,...</td>\n      <td>Photo by á„†á…µá†«á„‹á…§á†¼(minyoung) in ì•„ì›ƒë°±ìŠ¤í…Œì´í¬ë¶€ì²œì—­ì‚¬ì  with...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-03-27T16:05:40.000Z</td>\n      <td>hattie_sweet</td>\n      <td>hattie_sweet</td>\n      <td>CM7aIALMjWS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#ì»¤í”¼ë¨¸ì‹ ì´ìˆì–´ë„ #ì¹´í˜ê°„ë“œì•„ #ì•„ë©”í•˜ë£¨ìµœì†Œë‘ì” #í˜ˆì¤‘ì¹´í˜ì¸ë†ë„ìœ ì§€ #ëŒ€í•™ì›ìƒì¼ê¸° ...</td>\n      <td>NaN</td>\n      <td>Photo by Hattie_sweets on March 27, 2021. May ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-11-27T13:48:12.000Z</td>\n      <td>yoon_liana</td>\n      <td>yoon_liana</td>\n      <td>CIGLAGgHnH2</td>\n      <td>ì‹ ì´Œ Sinchon, Seoul, South Korea</td>\n      <td>sinchon-seoul-south-korea</td>\n      <td>486650968033082</td>\n      <td>58</td>\n      <td>NaN</td>\n      <td>ì–¼ë¥¸ ê±°ë¦¬ë‘ê¸°ì™„í™”ë˜ì„œ ë¶„ìœ„ê¸°ì¢‹ì€ ì¹´í˜ì—ì„œ ê·¸ë¦¼ ê·¸ë¦¬ê³  ì‹¶ë‹¤ğŸ™€\\n.\\n#coronao...</td>\n      <td>[#coronaout, #daily, #ëŒ€í•™ì›ìƒì¼ê¸°]</td>\n      <td>Photo by ë°•ì§€ìœ¤ in ì‹ ì´Œ Sinchon, Seoul, South Korea...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-11-27T12:37:04.000Z</td>\n      <td>archae_07</td>\n      <td>archae_07</td>\n      <td>CIGC3LDjJlJ</td>\n      <td>í•œêµ­ì „í†µë¬¸í™”ëŒ€í•™êµ</td>\n      <td>287114523</td>\n      <td>locations</td>\n      <td>19</td>\n      <td>NaN</td>\n      <td>í™ˆì¹´í˜ë§ê³  ë©ì¹´í˜â˜•\\n#ëŒ€í•™ì›ìƒì¼ê¸° #ì—°êµ¬ì‹¤ìŠ¤íƒ€ê·¸ë¨</td>\n      <td>[#ëŒ€í•™ì›ìƒì¼ê¸°, #ì—°êµ¬ì‹¤ìŠ¤íƒ€ê·¸ë¨]</td>\n      <td>Photo by ĞšĞ¸Ğ¼ Ñ‘Ğ½ Ğ³Ğ¸Ğ»ÑŒ in í•œêµ­ì „í†µë¬¸í™”ëŒ€í•™êµ. May be an i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-11-22T02:11:32.000Z</td>\n      <td>study_platypus</td>\n      <td>study_platypus</td>\n      <td>CH4DH1EDxOK</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>53 views</td>\n      <td>22 November 2020\\nğŸ˜ ë‹¥ì¹˜ê³  í•  ë¿ íƒ€ë‹¥íƒ€íƒ\\n.\\n.\\n.\\n#ph...</td>\n      <td>[#phdlife, #phdjourney, #ëŒ€í•™ì›ìƒì˜ì‚¶, #ë°•ì‚¬ê³¼ì •, #ëŒ€í•™ì›ìƒ,...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>2018-06-14T02:57:43.000Z</td>\n      <td>minjusm</td>\n      <td>minjusm</td>\n      <td>Bj_ThAbnfii</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>66</td>\n      <td>NaN</td>\n      <td>.\\n#ê´‘ê³ ë¡  #ë°œí‘œ\\në°œí‘œ ëë‚œ ì§€ í•˜ë£¨ ë§Œì— ë˜ ë°œí‘œ,\\nì£¼ë³€ ìƒí™©ì„ ë³´ë‹ˆ ë­ë¼...</td>\n      <td>NaN</td>\n      <td>Photo by á„‹á…¡á„‚á…¡á„‹á…®á†«á„‰á…¥ á„€á…µá†·á„†á…µá†«á„Œá…® on June 13, 2018. ...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>2017-08-12T16:13:09.000Z</td>\n      <td>nitrodiary</td>\n      <td>nitrodiary</td>\n      <td>BXszTFUhSOZ</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>#workingonweekends #nitrodiary #phdlife #ëŒ€í•™ì›ìƒì¼ê¸°</td>\n      <td>[#workingonweekends, #nitrodiary, #phdlife, #ëŒ€...</td>\n      <td>Photo by NitroD on August 12, 2017.</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>2017-08-11T15:15:02.000Z</td>\n      <td>nitrodiary</td>\n      <td>nitrodiary</td>\n      <td>BXqH2iOBwfA</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>ìº¬ #nitrodiary #phdlife #ëŒ€í•™ì›ìƒì¼ê¸°</td>\n      <td>[#nitrodiary, #phdlife, #ëŒ€í•™ì›ìƒì¼ê¸°]</td>\n      <td>Photo by NitroD on August 11, 2017.</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>2017-08-11T15:13:03.000Z</td>\n      <td>nitrodiary</td>\n      <td>nitrodiary</td>\n      <td>BXqHoEIhoQ1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>sunset on the Libe Slope #throwback #cornell #...</td>\n      <td>[#throwback, #cornell, #memories, #nitrodiary,...</td>\n      <td>Photo by NitroD on August 11, 2017.</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>2017-08-01T19:57:11.000Z</td>\n      <td>nitrodiary</td>\n      <td>nitrodiary</td>\n      <td>BXQ4MaaBXqV</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>#nitrodiary #manuscript #submitted #phdlife #ëŒ€...</td>\n      <td>[#nitrodiary, #manuscript, #submitted, #phdlif...</td>\n      <td>Photo by NitroD on August 01, 2017.</td>\n    </tr>\n  </tbody>\n</table>\n<p>62 rows Ã— 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "# display just a couple of rows\n",
    "df\n",
    "\n",
    "# display all rows\n",
    "# pd.set_option('display.max_rows', df.shape[0]+1) # display all rows\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}